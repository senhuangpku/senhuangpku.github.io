<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Useful Links</title>
    <url>/posts/7f104436/</url>
    <content><![CDATA[<img src="/posts/7f104436/0709BF26-C33C-4D44-AD23-B0722DF16D04_1_201_a.jpeg" width="30%" height="30%">

<ul>
<li>Latex<br><a href="https://www.cnblogs.com/1024th/p/11623258.html">Latex公式大全</a></li>
</ul>
]]></content>
      <tags>
        <tag>Links</tag>
      </tags>
  </entry>
  <entry>
    <title>MOT Basic</title>
    <url>/posts/7ec54567/</url>
    <content><![CDATA[<h1 id="MOT"><a href="#MOT" class="headerlink" title="MOT"></a>MOT</h1><h2 id="研究现状"><a href="#研究现状" class="headerlink" title="研究现状"></a>研究现状</h2><p>目标跟踪可分为单目标跟踪和多目标跟踪，多目标跟踪又分为基于检测的跟踪和不基于检测的跟踪，前者的解释性和跟踪效果更好，成为学界和业界的主流方法；从是否利用未来帧的信息又可分为在线和离线两种跟踪方式。</p>
<ul>
<li><h3 id="单目标跟踪"><a href="#单目标跟踪" class="headerlink" title="单目标跟踪"></a>单目标跟踪</h3>  单目标跟踪常用于刑侦、军事等领域，特点是感兴趣目标单一，对跟踪的连贯性要求较高，这一领域的算法主要有生成式法和判别式法。生成式法旨在通过特定的模型来获取目标的信息，在通过最小化已有的匹配误差来实现目标跟踪；判别式法则近似于二分类问题，其利用提前训练好的分类器来区分前景和背景，该方法的跟踪性能更好，这一方法也是单目标跟踪领域的主流方法。</li>
<li><h3 id="多目标跟踪"><a href="#多目标跟踪" class="headerlink" title="多目标跟踪"></a>多目标跟踪</h3>  多目标跟踪任务通常可分为目标特征提取、目标关联匹配和轨迹后处理。其中，目标特征提取部分专注于提取跟踪目标的特征，所提取的特征主要用于区分场景中的多个目标和计算其相似程度。传统特征提取包括表观特征和运动特征。一般而言，同一目标的外观在视频序列的前后变化是均匀且缓慢的，其在相邻帧间的位置也是相近的。目标关联匹配正是利用多目标之间的相似度来进行帧序列目标的关联匹配，通过关联优化算法根据多个目标节点的相似程度对具有高度相似的节点进行关联，并在时间序列形成轨迹。由于跟踪场景的复杂多变，跟踪算法形成的轨迹信息通常会是有错误的，轨迹后处理方法则是对原有轨迹进行优化和修正，以提高跟踪效果。<ul>
<li>评价指标<br>  真实的正样本（TP）、本真实的副样本（TN）、错误的正样本（FP）、错误的负样本（FN）、编号跳变（IDSw）、轨迹碎片数（Frag）<br>  准确度：表示预测结果的精确度，预测正确的样本数除以总样本数<br>  $$A &#x3D; \frac{TP + TN}{TP + TN + FP + FN} $$<br>  精确度：又称为查准率，表示预测结果中，预测为正样本的样本中，正确预测为正样本的概率$$P &#x3D; \frac{TP}{TP + FP}$$<br>  召回率：又称为查全率，表示在原始样本的正样本中，最后被正确预测为正样本的概率$$R &#x3D; \frac{TP}{TP + FN}$$<br>  多目标跟踪准确率：$MOTA &#x3D; 1 - \frac{FP + FN + IDSw}{GT}$  $GT$表示场景中所有真实存在的目标数。<br>  多目标跟踪精确度：$MOTP &#x3D;\frac {\sum_{i,t}d_{t}^{i}}{\sum_{t}c_{t}}$ $c_{t}$表示预测结果于真实结果在第t帧所匹配到的个数，$d_{t}^{i}$表示每个匹配对中预测结果与真实结果的距离误差。<br>  编号精确度：表示跟踪过程中每个目标在每个时刻对已匹配节点$ID$编号的精确度$$IDP &#x3D; \frac{IDTP}{IDTP + IDFP}$$其中$IDTP$表示轨迹中编号正确的边界框数量，$IDFP$表示所提供跟踪结果除$IDTP$之外的所有边界框数量，即在基准编号外所有已匹配边界框数量及错误样本边界框数量之和。<br>  编号召回率：表示跟踪过程中每个目标在每个时刻对已匹配节点$ID$编号的召回率<br>  $$IDR&#x3D;\frac{IDTP}{IDTP + IDFN}$$其中$IDFN$表示真实场景中除$IDTP$之外的所有边界框数量，即在基准编号且与真实匹配的边界框之外所有真实轨迹未匹配的边界框数量。<br>  编号$F1-Score(IDF1)$:表示跟踪过程中每个目标在每时刻对节点$ID$编号的$F1$值：<br>  $$IDF1 &#x3D; \frac{2 • IDTP}{2*IDTP+IDFP+IDFN}&#x3D; 2•\frac{IDP • IDR}{IDP+IDR}$$<br>  <img src="/posts/7ec54567/%E6%88%AA%E5%B1%8F2022-06-16%2016.48.58.png" alt="图1"><span class="image-caption">图1</span></li>
</ul>
</li>
<li><h3 id="国内外研究现状综述"><a href="#国内外研究现状综述" class="headerlink" title="国内外研究现状综述"></a>国内外研究现状综述</h3>  早在上个世纪70年代末，Porter就己开展了在多目标跟踪领域的研究，利用卡尔曼滤波的方式来描述物体的运动方式，同时利用统计似然函数计算每个可能与对象的关联假设来关联跟踪对象。Jianbo等人在1994年通过提出的一种高鲁棒性的Shi-Tomas特征角点来对特定位置进行跟踪，该特征点在跟踪过程中对尺度、旋转等问题具有高鲁棒性。随着计算机视觉技术的整体提升，更多的技术可以应用到现实场景中，尤其在智能监控、自动驾驶等领域对多目标跟踪技术的需求迫切，因此该领域在近些年来的相关工作与日俱增。<strong>Wenhan、Junliang、Anton</strong>等人在文章中对多目标跟踪领域进行了综述，并在每年对文章进行实时更新。<br>  多目标跟踪任务可以被看作一个多变量预测问题，给定视频序列，假设场景中的目标$s_{t}^{i}$表示在第$t$帧的第$i$个目标，则在第$t$帧的目标集合可表示为$S_{t}&#x3D; (s_{t}^{1},s_{t}^{2},…,s_{t}^{M_{t}})$,其中$M_{t}$表示当前帧所有的目标数量。对于目标$s_{t}^{i}$在场景中所形成的轨迹可被定义为$s_{i_{s}:i_{e}}^{i}&#x3D;{s_{i_{s}}^{i},…,s_{i_{e}}^{i}}$，其中$i_s$和$i_e$分别表示该目标所对应轨迹的起始和终止时间。$S_{1:t}&#x3D; {S_1,S_2,…,S_t}$表示从第1帧到第$t$帧的所有轨迹结果。前几年，多目标跟踪任务的处理流程大致分为两类，如图2.1所示：<br>  <img src="/posts/7ec54567/%E6%88%AA%E5%B1%8F2022-06-16%2019.59.07.png" alt="图2"><span class="image-caption">图2</span><br>  多目标跟踪任务需要对场景中所有的目标进行跟踪，而视觉跟踪任务只需要考虑单一目标的特征形态变换和运动轨迹预测以及其他目标对跟踪目标的影响。而多目标跟踪任务需要考虑场景中所有目标的特征及其相互关系，然而跟踪方法在执行过程中并不能获得场景中目标的具体数量以及目标进场离场情况，因此多目标跟踪的核心思路是<strong>通过对视频序列的逐帧或隔帧进行目标检测</strong>，通过检测结果的边界框来进行目标关联(DataAssociation）来实现对多个目标的跟踪，该框架也可称之为基于检测的跟踪任务(Tracking-by-Detection)。</li>
</ul>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><ul>
<li><h3 id="初始化方法"><a href="#初始化方法" class="headerlink" title="初始化方法"></a>初始化方法</h3><p>  DBT 和 DFT两种<br>  <img src="/posts/7ec54567/%E6%88%AA%E5%B1%8F2022-06-24%2019.19.31.png" alt="图3"><span class="image-caption">图3</span></p>
</li>
<li><h3 id="处理模式"><a href="#处理模式" class="headerlink" title="处理模式"></a>处理模式</h3><p>  在线追踪和离线追踪</p>
</li>
<li><h3 id="输出类型"><a href="#输出类型" class="headerlink" title="输出类型"></a>输出类型</h3><p>  确定性方法和概率方法</p>
</li>
</ul>
<h2 id="MOT-Components"><a href="#MOT-Components" class="headerlink" title="MOT Components"></a>MOT Components</h2><p>MOT的目标是在给定视频中解决单帧多目标识别和连续帧的目标关联问题，开发MOT算法主要包含两部分：一是如何去衡量帧内的目标的相似度；二是如何去比较连续帧内物体的相似度。</p>
<ul>
<li><h3 id="Apperance-Model"><a href="#Apperance-Model" class="headerlink" title="Apperance Model"></a>Apperance Model</h3><p>  外观模型包含两部分：视觉表达和统计测量，视觉表达描述利用一些特征描述物体的的视觉特点，统计测量则是计算不同观测物的相似度。</p>
<ul>
<li>视觉表达<br>  视觉表达主要包括局部特征、区域特征和其他特征。<br>  <img src="/posts/7ec54567/%E6%88%AA%E5%B1%8F2022-06-26%2017.28.23.png" alt="图4"><span class="image-caption">图4</span></li>
<li>统计测量<br>  基于视觉表达所描述的观测物体特征，统计测量计算两观测物体间的相似度，其方法可大致分为基于单特征和基于多特征，其中给予多特征的主要方法如图所示。<br>  <img src="/posts/7ec54567/%E6%88%AA%E5%B1%8F2022-06-26%2017.33.30.png" alt="图5"><span class="image-caption">图5</span></li>
</ul>
</li>
<li><h3 id="Motion-Model"><a href="#Motion-Model" class="headerlink" title="Motion Model"></a>Motion Model</h3><p>  运动模型负责捕捉物体的运动行为，他估计物体未来帧中的潜在位置，因此可以减小搜索区域。其主要分为线性运动模型和非线性运动模型。</p>
<ul>
<li>线性运动模型<br>  线性运动模型是目前使用范围最广的运动模型，该类模型应用匀速运动的基本假设，基于此假设有三种模型构建方案：位置平滑、速度平滑和加速度平滑。</li>
<li>非线性运动模型<br>  非线性运动模型用于更加精确的预估物体运动状态，但会消耗更大的计算资源。<br>  <img src="/posts/7ec54567/%E6%88%AA%E5%B1%8F2022-06-26%2022.42.15.png" alt="图6"><span class="image-caption">图6</span></li>
</ul>
</li>
<li><h3 id="Interaction-Model"><a href="#Interaction-Model" class="headerlink" title="Interaction Model"></a>Interaction Model</h3><p>  交互模型是考虑不同物体之间相互作用的模型，主要有社会力模型和人群运动模式模型。</p>
<ul>
<li>社会力模型<br>  社会力模型也成为群体模型，此模型中，每个物体都其他个体和环境因素。在社会力模型中，基于个体力和群体力两个层面对个体行为进行建模。<br>  <img src="/posts/7ec54567/%E6%88%AA%E5%B1%8F2022-06-27%2013.47.59.png" alt="图7"><span class="image-caption">图7</span></li>
<li>人群运动模式模型<br>  在拥挤的群体环境中，群体的运动特征较外观特征更具有可靠性，粗略的来讲分为结构化和非结构化运动模式。</li>
</ul>
</li>
<li><h3 id="Exclusion-Model"><a href="#Exclusion-Model" class="headerlink" title="Exclusion Model"></a>Exclusion Model</h3><p>  排斥模型旨在通过禁止无力碰撞来寻求多目标追踪的解决方案，包括检测级排斥模型和轨迹级排斥模型。</p>
<ul>
<li>检测级排斥模型<br>  检测级排斥模型分为软建模和硬建模两种，软建模通过最小化损失函数来实现约束，硬建模通过显式约束的方式进行建模。</li>
<li>轨迹级排斥模型<br>  通常，轨迹级排除是通过惩罚两个接近检测假设具有不同轨迹标签的情况来建模的。</li>
</ul>
</li>
<li><h3 id="遮挡处理"><a href="#遮挡处理" class="headerlink" title="遮挡处理"></a>遮挡处理</h3><p>  遮挡可能是 MOT 中最关键的挑战。它是 ID 切换或轨迹碎片化的主要原因。</p>
<ul>
<li>局部到整体<br>  此策略基于物体不会被完全遮挡的假设，基于此假设，通过观察到的物体局部来推理物体的整体状态。</li>
<li>假设和测试<br>  首先基于观察到的信息进行遮挡假设，然后利用假设信息进行测试，分为遮挡分散注意力和遮挡辅助检测两种。</li>
<li>缓冲和恢复<br>  这种策略在遮挡发生时缓冲观察并记住遮挡前对象的状态。当遮挡结束时，对象状态会根据缓存的观测值和遮挡前存储的状态恢复。</li>
<li>其他<br>  一般来说，明确区分或分类各种遮挡建模方法并非易事，并且在某些情况下，多种策略组合使用。</li>
</ul>
</li>
<li><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul>
<li><p><strong>概率推理</strong><br>  跟踪算法的目标是在现有观测的基础上，通过多种概率推理方法估计目标状态的概率分布。这种方法通常只需要现有的，即过去和现在的观察，因此它们特别适合在线跟踪的任务。<br>  各种概率推理模型已应用于多目标跟踪，例如卡尔曼滤波器、扩展卡尔曼滤波器和粒子滤波器。<br>  <img src="/posts/7ec54567/%E6%88%AA%E5%B1%8F2022-06-27%2016.14.37.png"></p>
</li>
<li><p><strong>确定性优化</strong><br>  与概率推理方法相反，基于确定性优化的方法旨在找到 MOT 的最大后验 (MAP) 解决方案。该框架内的方法更适合于离线跟踪任务，因为需要提前获得来自所有帧或至少一个时间窗口的观察结果。</p>
<ul>
<li>二部图匹配<br>  通过将 MOT 问题建模为二分图匹配，两组不相交的图节点可以是在线跟踪中的现有轨迹和新检测，也可以是在线跟踪中的两组轨迹。</li>
<li>动态规划<br>  扩展动态规划、线性规划、二次布尔规划、K-最短路径、设置覆盖和子图多割，采用最大多团来解决检测或轨迹之间的关联问题。</li>
<li>最小成本最大流量网络流量<br>  网络流是一个有向图，其中每条边都有一定的容量。对于 MOT 图中的节点是检测响应或轨迹。流被建模为连接两个节点与否的指标。请注意，全局最优解可以在多项式时间内获得，例如使用 push-relabel 算法。该模型得到了广泛的应用。</li>
<li>条件随机场<br>  定义一个图 G &#x3D; (V, E)，其中 V 是节点集，E 是边集，低级轨迹作为图的输入。图中的每个节点代表观察或轨迹对，并且预测标签以指示观察属于哪个轨迹或是否链接轨迹。</li>
<li>最大权重独立集<br>  最大权重独立集 (MWIS) 是属性图的不相邻节点的最重子集。与上面描述的 CRF 模型一样，属性图中的节点表示连续帧中的轨迹对，节点的权重表示轨迹对的匹配度，如果两个轨迹共享相同的检测，则边是连接的。鉴于此图，数据关联被建模为 MWIS 问题。</li>
</ul>
<p>  在实践中，与概率方法相比，更普遍地采用确定性优化或能量最小化。尽管概率方法为问题提供了更直观和完整的解决方案，但它们通常难以推断。相反，能量最小化可以在合理的时间内获得“足够好”的解决方案。<br>  一般来说，外观、运动和推理在大多数方法中都是强制性的。还值得注意的是，这些组件彼此不正交。它们通常可以组合和集成以获得令人满意的性能。</p>
</li>
</ul>
</li>
</ul>
<h2 id="MOT-Evaluation"><a href="#MOT-Evaluation" class="headerlink" title="MOT Evaluation"></a>MOT Evaluation</h2><p>MOT 指标可以大致分为两组，分别评估检测和跟踪，如表 7 所示。<br><img src="/posts/7ec54567/%E6%88%AA%E5%B1%8F2022-06-27%2020.18.34.png"></p>
<h3 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h3><ul>
<li>Metrics for detection<br>我们进一步将检测指标分为两个子集。一组测量准确性，另一组测量精度。</li>
<li>Metrics for tracking<br>准确度、精度、完整性、鲁棒性。</li>
</ul>
<h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p><img src="/posts/7ec54567/%E6%88%AA%E5%B1%8F2022-06-27%2021.25.16.png"></p>
]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>MOT 基础知识</tag>
      </tags>
  </entry>
</search>
